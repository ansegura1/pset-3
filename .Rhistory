##################################Angela Segura Cod. 201619956   R version 4.1.3 ############################################
##Taller Pset_3####
###Crear directorio de trabajo##
dir.create("code") #Creamos carpeta llamada code, donde guardaremos el script
dir.create("output") #Creamos la carpeta output para los datos procesados
list.files() ## Revisamos con que carpetas estamos trabajando#
##El script lo guardamos manualmente en la carpeta de code con el nombre Scriptrespuesta##
###Instalar librerias##
require(pacman)
p_load(tidyverse, rio,skimr,tmaptools, sf, leaflet, rvest, xml2, osmdata, ggsn) # Llamamos las librerias  necesarias para el taller##
###Instalar librerias##
require(pacman)
p_load(tidyverse, rio,skimr,tmaptools, sf, leaflet, rvest, xml2, osmdata, ggsn) # Llamamos las librerias  necesarias para el taller##
#########################################1.Regresiones#######################################################
####################################################2. Datos espaciales######################################################
## 2.1  Descargar los datos##
available_features() %>% head(100)
available_tags("amenity") %>% head(20)
opq(bbox = getbb("Armenia Colombia")) # Vamos a obtener la caja de coordenada que contiene el polÃ­gono de Armenia##
##################################Angela Segura Cod. 201619956   R version 4.1.3 ############################################
##Taller Pset_3####
###Crear directorio de trabajo##
dir.create("code") #Creamos carpeta llamada code, donde guardaremos el script
dir.create("output") #Creamos la carpeta output para los datos procesados
list.files() ## Revisamos con que carpetas estamos trabajando#
##El script lo guardamos manualmente en la carpeta de code con el nombre Scriptrespuesta##
###Instalar librerias##
require(pacman)
p_load(tidyverse, rio,skimr,tmaptools, sf, leaflet, rvest, xml2, osmdata, ggsn) # Llamamos las librerias  necesarias para el taller##
#########################################1.Regresiones#######################################################
####################################################2. Datos espaciales######################################################
## 2.1  Descargar los datos##
available_features() %>% head(100)
available_tags("amenity") %>% head(20)
opq(bbox = getbb("Armenia Colombia")) # Vamos a obtener la caja de coordenada que contiene el polÃ­gono de Armenia##
## Datos de los restaurantes en Armenia
osm = opq(bbox = getbb("Armenia Colombia")) %>%
add_osm_feature(key="amenity" , value="restaurant") #En internet encontramos cÃ³mo encontrar los datos de los restaurantes en openstreetmap. (https://wiki.openstreetmap.org/wiki/Tag:amenity%3Drestaurant)
class(osm)
osm_sf = osm %>% osmdata_sf() ## extraer Simple Features Collection
osm_sf
restaurant = osm_sf$osm_points %>% select(osm_id,amenity)#Obtenemos el objeto
## Parques de Armenia
parques <- opq(bbox = getbb("Armenia Colombia")) %>%
add_osm_feature(key = "leisure", value = "park") %>%
osmdata_sf() %>% .$osm_polygons %>% select(osm_id,name)
## 2.2 Visualizar  los datos##
#restaurantes
leaflet() %>% addTiles() %>% addCircles(data=restaurant , col="red")
#parques
leaflet() %>% addTiles() %>% addPolygons(data=parques, col="blue")
#2.3 Geocodificar direcciones##
#Museo del oro Qumbaya
MOQ <- geocode_OSM("Museo del Oro Quimbaya, Armenia", as.sf=T)
#Pintar el punto del museo del Oro Quimbaya
leaflet() %>% addTiles() %>% addCircleMarkers(data=MOQ , col="yellow")
#2.4 Exportar mapa##
###################################3. Web-scraping y procesamiento de texto#####################################################
library(rvest)
browseURL("https://es.wikipedia.org/wiki/Departamentos_de_Colombia") #Para el punto 3 vamos a usar la pagina de departamentos de Colombia
vignette("rvest")
##3.1 Crear objeto html##
my_url = "https://es.wikipedia.org/wiki/Departamentos_de_Colombia" #Guardo la url de la pagina
browseURL(my_url)
my_html = read_html(my_url)#Guardo la Html
class(my_html) ## verificar que quedÃ³ guardada   como un objeto xml_document#
##3.2  extraer el tÃ­tulo de la pÃ¡gina##
my_html %>% html_nodes(xpath = '//*[@id="firstHeading"]/span') %>% #En la pag le damos inspeccionar al elemento y posteriormente le damos copiar Xpath
html_text() #Visualizamos el elemento extraido
#3.4 Extraer los parrafos del documento (elementos con etiqueta p) y generar nube de palabras##
Parrafos<- my_html %>% html_elements("p") %>% html_text() ### Ver los textos que estan con la etiqueta p
Parrafos
my_table = my_html %>% html_table()
library(rvest)
browseURL("https://es.wikipedia.org/wiki/Departamentos_de_Colombia") #Para el punto 3 vamos a usar la pagina de departamentos de Colombia
vignette("rvest")
##3.1 Crear objeto html##
my_url = "https://es.wikipedia.org/wiki/Departamentos_de_Colombia" #Guardo la url de la pagina
browseURL(my_url)
my_html = read_html(my_url)#Guardo la Html
class(my_html) ## verificar que queda guardada   como un objeto xml_document#
##3.2  extraer el titulo de la pagina##
my_html %>% html_nodes(xpath = '//*[@id="firstHeading"]/span') %>% #En la pag le damos inspeccionar al elemento y posteriormente le damos copiar Xpath
html_text() #Visualizamos el elemento extraido
my_table = my_html %>% html_table()
View(my_table)
View(my_table[[5]])
View(my_table[[4]])
departamentos= my_table[[4]]
my_table = my_html %>% html_table() #Descargo todas las tablas de la pagina
View(my_table) #Observo cuantas filas y columnas tienen las tablas descargadas para encontrar la de departamentos
View(my_table[[4]]) #Verificamos que la tabla 5 corresponde a la tabla de departamentos
departamentos= my_table[[4]]
export(x=departamentos , file="output/tabla_departamento.xlsx")
library(rio)
library(rio)
my_table = my_html %>% html_table() #Descargo todas las tablas de la pagina
View(my_table) #Observo cuantas filas y columnas tienen las tablas descargadas para encontrar la de departamentos
View(my_table[[4]]) #Verificamos que la tabla 5 corresponde a la tabla de departamentos
departamentos= my_table[[4]]
export(x=departamentos , file="output/tabla_departamento.xlsx")
stop_words_1<- get_stopwords(language = "es") #Cargar stop words
library(dplyr)
stop_words_1<- get_stopwords(language = "es") #Cargar stop words
library(tidytext)
stop_words_1<- get_stopwords(language = "es") #Cargar stop words
stop_words_1<- get_stopwords(language = "en") #Cargar stop words
View(stop_words_1)
stop_words_1<- get_stopwords(language = "es")
View(stop_words_1)
text <- data$text
as.character(Parrafos)
docs <- Corpus(VectorSource(Parrafos))
install.packages("tm")
library(tm)
install.packages("C:/Users/huawe/Downloads/NLP_0.2-0.tar.gz", repos = NULL, type = "source")
docs <- Corpus(VectorSource(Parrafos))
View(docs)
docs[["7"]]
docs[["15"]]
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
library(wordcloud)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
View(docs)
View(stop_words_1)
docs <- Corpus(VectorSource(Parrafos)) %>%
anti_join(stop_words_1, by = c("palabra"="words")) %>%
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,
Parrafos<- my_html %>% html_elements("p") %>% html_text() ### Ver los textos que estan con la etiqueta p
Parrafos
as.character(Parrafos)
install.packages("tm")
library(tm)
library(wordcloud)
library(stopwords)
install.packages("tm")
stop_words_1<- get_stopwords(language = "es")
docs <- Corpus(VectorSource(Parrafos))
library(tm)
stop_words_1<- get_stopwords(language = "es")
docs <- Corpus(VectorSource(Parrafos))
docs2 <-docs  %>% anti_join(stop_words_1, by = c("palabra"="words"))
docs2
docs2 <-docs  %>% anti_join(stop_words_1, by = c("palabra"="words"))
library(dplyr)
stop_words_1<- get_stopwords(language = "es")
docs <- Corpus(VectorSource(Parrafos))
docs2 <-docs  %>% anti_join(stop_words_1, by = c("palabra"="words"))
docs <- Corpus(VectorSource(Parrafos)) %>% anti_join(stop_words_1, by = c("palabra"="words"))
docs <- Corpus(VectorSource(Parrafos)) %>% anti_join(stop_words_1, by = c("palabra"="words"))
docs <- Corpus(VectorSource(Parrafos)) %>% anti_join(stop_words_1, by = c("palabra"="words"))
Parrafos<- my_html %>% html_elements("p") %>% html_text() ### Ver los textos que estan con la etiqueta p
Parrafos
as.character(Parrafos)
install.packages("tm")
library(tm)
library(wordcloud)
library(stopwords)
install.packages("tm")
library(dplyr)
stop_words_1<- get_stopwords(language = "es")
library(stopwords)
stop_words_1<- get_stopwords(language = "es")
Parrafosc <-Parrafos%>% anti_join(stop_words_1, by = c("palabra"="words"))
View(stop_words_1)
docs <- Corpus(VectorSource(Parrafo))
library(tidyr)
docs <- Corpus(VectorSource(Parrafo))
library(tm)
docs <- Corpus(VectorSource(Parrafo))
docs <- Corpus(VectorSource(Parrafos))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("spanish"))
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
library(wordcloud)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=100, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=50, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=50, random.order=FALSE, rot.per=0.25,            colors=brewer.pal(8, "Dark2"))
# 3B: Nubes de palabras y bigramas
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=100, random.order=FALSE, rot.per=0.25,            colors=brewer.pal(8, "Dark2"))
png("output/nube_palabras.png")
df <- data.frame(word = names(words),freq=words)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=100, random.order=FALSE, rot.per=0.25,            colors=brewer.pal(8, "Dark2"))
pdf("output/nube_palabras.pdf")
##################################Angela Segura Cod. 201619956   R version 4.1.3 ############################################
##Taller Pset_3####
###Crear directorio de trabajo##
dir.create("code") #Creamos carpeta llamada code, donde guardaremos el script
dir.create("output") #Creamos la carpeta output para los datos procesados
list.files() ## Revisamos con que carpetas estamos trabajando#
##El script lo guardamos manualmente en la carpeta de code con el nombre Scriptrespuesta##
###Instalar librerias##
require(pacman)
p_load(tidyverse, rio,skimr,tmaptools, sf, leaflet, rvest, xml2, osmdata, ggsn) # Llamamos las librerias  necesarias para el taller##
#########################################1.Regresiones#######################################################
####################################################2. Datos espaciales######################################################
## 2.1  Descargar los datos##
available_features() %>% head(100)
available_tags("amenity") %>% head(20)
opq(bbox = getbb("Armenia Colombia")) # Vamos a obtener la caja de coordenada que contiene el polÃ­gono de Armenia##
## Datos de los restaurantes en Armenia
osm = opq(bbox = getbb("Armenia Colombia")) %>%
add_osm_feature(key="amenity" , value="restaurant") #En internet encontramos cÃ³mo encontrar los datos de los restaurantes en openstreetmap. (https://wiki.openstreetmap.org/wiki/Tag:amenity%3Drestaurant)
class(osm)
osm_sf = osm %>% osmdata_sf() ## extraer Simple Features Collection
osm_sf
restaurant = osm_sf$osm_points %>% select(osm_id,amenity)#Obtenemos el objeto
## Parques de Armenia
parques <- opq(bbox = getbb("Armenia Colombia")) %>%
add_osm_feature(key = "leisure", value = "park") %>%
osmdata_sf() %>% .$osm_polygons %>% select(osm_id,name)
## 2.2 Visualizar  los datos##
#restaurantes
leaflet() %>% addTiles() %>% addCircles(data=restaurant , col="red")
#parques
leaflet() %>% addTiles() %>% addPolygons(data=parques, col="blue")
#2.3 Geocodificar direcciones##
#Museo del oro Qumbaya
MOQ <- geocode_OSM("Museo del Oro Quimbaya, Armenia", as.sf=T)
#Pintar el punto del museo del Oro Quimbaya
leaflet() %>% addTiles() %>% addCircleMarkers(data=MOQ , col="yellow")
#2.4 Exportar mapa##
###################################3. Web-scraping y procesamiento de texto#####################################################
library(rvest)
browseURL("https://es.wikipedia.org/wiki/Departamentos_de_Colombia") #Para el punto 3 vamos a usar la pagina de departamentos de Colombia
vignette("rvest")
##3.1 Crear objeto html##
my_url = "https://es.wikipedia.org/wiki/Departamentos_de_Colombia" #Guardo la url de la pagina
browseURL(my_url)
my_html = read_html(my_url)#Guardo la Html
class(my_html) ## verificar que queda guardada   como un objeto xml_document#
##3.2  extraer el titulo de la pagina##
my_html %>% html_nodes(xpath = '//*[@id="firstHeading"]/span') %>% #En la pag le damos inspeccionar al elemento y posteriormente le damos copiar Xpath
html_text() #Visualizamos el elemento extraido
#3.3 Extraer la tabla que contiene los departamentos de Colombia y exportar
library(rio)
my_table = my_html %>% html_table() #Descargo todas las tablas de la pagina
View(my_table) #Observo cuantas filas y columnas tienen las tablas descargadas para encontrar la de departamentos
View(my_table[[4]]) #Verificamos que la tabla 5 corresponde a la tabla de departamentos
departamentos= my_table[[4]]
export(x=departamentos , file="output/tabla_departamento.xlsx")
#3.4 Extraer los parrafos del documento (elementos con etiqueta p) y generar nube de palabras##
Parrafos<- my_html %>% html_elements("p") %>% html_text() ### Ver los textos que estan con la etiqueta p
Parrafos
as.character(Parrafos)
install.packages("tm")
library(tm)
library(wordcloud)
library(stopwords)
library(dplyr)
docs <- Corpus(VectorSource(Parrafos))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("spanish"))
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.25,colors=brewer.pal(8, "Dark2"))
png("output/nube_palabras.png")
install.packages("tm")
##################################Angela Segura Cod. 201619956   R version 4.1.3 ############################################
##Taller Pset_3####
###Crear directorio de trabajo##
dir.create("code") #Creamos carpeta llamada code, donde guardaremos el script
dir.create("output") #Creamos la carpeta output para los datos procesados
list.files() ## Revisamos con que carpetas estamos trabajando#
##El script lo guardamos manualmente en la carpeta de code con el nombre Scriptrespuesta##
###Instalar librerias##
require(pacman)
install.packages("tm")
install.packages("wordcloud")
p_load(tidyverse, rio,skimr,tmaptools, sf, leaflet, rvest, xml2, osmdata, ggsn, tm,wordcloud,dplyr) # Llamamos las librerias  necesarias para el taller##
#########################################1.Regresiones#######################################################
####################################################2. Datos espaciales######################################################
## 2.1  Descargar los datos##
available_features() %>% head(100)
available_tags("amenity") %>% head(20)
opq(bbox = getbb("Armenia Colombia")) # Vamos a obtener la caja de coordenada que contiene el polÃ­gono de Armenia##
## Datos de los restaurantes en Armenia
osm = opq(bbox = getbb("Armenia Colombia")) %>%
add_osm_feature(key="amenity" , value="restaurant") #En internet encontramos cÃ³mo encontrar los datos de los restaurantes en openstreetmap. (https://wiki.openstreetmap.org/wiki/Tag:amenity%3Drestaurant)
class(osm)
osm_sf = osm %>% osmdata_sf() ## extraer Simple Features Collection
osm_sf
restaurant = osm_sf$osm_points %>% select(osm_id,amenity)#Obtenemos el objeto
## Parques de Armenia
parques <- opq(bbox = getbb("Armenia Colombia")) %>%
add_osm_feature(key = "leisure", value = "park") %>%
osmdata_sf() %>% .$osm_polygons %>% select(osm_id,name)
## 2.2 Visualizar  los datos##
#restaurantes
leaflet() %>% addTiles() %>% addCircles(data=restaurant , col="red")
#parques
leaflet() %>% addTiles() %>% addPolygons(data=parques, col="blue")
#2.3 Geocodificar direcciones##
#Museo del oro Qumbaya
MOQ <- geocode_OSM("Museo del Oro Quimbaya, Armenia", as.sf=T)
#Pintar el punto del museo del Oro Quimbaya
leaflet() %>% addTiles() %>% addCircleMarkers(data=MOQ , col="yellow")
#2.4 Exportar mapa##
###################################3. Web-scraping y procesamiento de texto#####################################################
browseURL("https://es.wikipedia.org/wiki/Departamentos_de_Colombia") #Para el punto 3 vamos a usar la pagina de departamentos de Colombia
##3.1 Crear objeto html##
my_url = "https://es.wikipedia.org/wiki/Departamentos_de_Colombia" #Guardo la url de la pagina
browseURL(my_url)
my_html = read_html(my_url)#Guardo la Html
class(my_html) ## verificar que queda guardada   como un objeto xml_document#
##3.2  extraer el titulo de la pagina##
my_html %>% html_nodes(xpath = '//*[@id="firstHeading"]/span') %>% #En la pag le damos inspeccionar al elemento y posteriormente le damos copiar Xpath
html_text() #Visualizamos el elemento extraido
#3.3 Extraer la tabla que contiene los departamentos de Colombia y exportar
my_table = my_html %>% html_table() #Descargo todas las tablas de la pagina
View(my_table) #Observo cuantas filas y columnas tienen las tablas descargadas para encontrar la de departamentos
View(my_table[[4]]) #Verificamos que la tabla 5 corresponde a la tabla de departamentos
departamentos= my_table[[4]]
export(x=departamentos , file="output/tabla_departamento.xlsx")
#3.4 Extraer los parrafos del documento (elementos con etiqueta p) y generar nube de palabras##
Parrafos<- my_html %>% html_elements("p") %>% html_text() ### Ver los textos que estan con la etiqueta p
Parrafos
as.character(Parrafos)
docs <- Corpus(VectorSource(Parrafos))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("spanish"))
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.25,colors=brewer.pal(8, "Dark2"))
png("output/nube_palabras.png")
wordcloud(words = df$word, freq = df$freq, min.freq = 5, max.words=100, random.order=FALSE, rot.per=0.25,colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.25,colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, freq = df$freq, min.freq = 2, max.words=100, random.order=FALSE, rot.per=0.25,colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, freq = df$freq, min.freq = 2, max.words=100, random.order=FALSE, rot.per=0.15,colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, freq = df$freq, min.freq = 2, max.words=100, random.order=FALSE, rot.per=0.15,colors=brewer.pal(8, "Dark2")) scale=c(4, .5)
wordcloud(words = df$word, freq = df$freq, min.freq = 2, max.words=100, random.order=FALSE, rot.per=0.15,colors=brewer.pal(8, "Dark2") [3:5])
png("output/nube_palabras.png")
wordcloud(words = df$word, freq = df$freq, min.freq = 2, max.words=50, random.order=FALSE, rot.per=0.15,colors=brewer.pal(8, "Dark2") [3:5])
png("output/nube_palabras.png")
